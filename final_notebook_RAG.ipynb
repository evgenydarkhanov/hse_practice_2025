{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df82c46-17f5-4463-a2a4-1d822be38a2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952349bc-68d3-4e74-8ce2-65627bcd7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface imports\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable\n",
    "\n",
    "# bm25 imports\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "import pymorphy3\n",
    "import numpy as np\n",
    "\n",
    "# embedding imports\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# hybrid imports\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class RetrieverInterface(ABC):\n",
    "    \"\"\"Интерфейс для поисковиков по базе знаний.\n",
    "\n",
    "    Все добавляемые поисковики должны наследоваться от него\n",
    "    и имплементировать метод `make_query`.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_query(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BM25Retriever(RetrieverInterface):\n",
    "    \"\"\"Статистический поиск по базе знаний.\n",
    "\n",
    "    Поиск с использованием разреженных векторов.\n",
    "    Алгоритм ранжирования: Okapi BM25.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 stopwords_path: str,\n",
    "                 documents: Iterable[str],\n",
    "                 token_pattern: str) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            stopwords_path: путь к стоп-словам для языка поиска.\n",
    "            documents: коллекция ключей, среди которых производится поиск.\n",
    "            token_pattern: паттерн для регулярного выражения.\n",
    "        \"\"\"\n",
    "\n",
    "        #: экстракция стоп-слов из файла\n",
    "        with open(stopwords_path, 'r') as file:\n",
    "            self._stopwords = {word.strip() for word in file}\n",
    "\n",
    "        self._lemmatizer = pymorphy3.MorphAnalyzer()\n",
    "        self._TOKEN_PATTERN = re.compile(token_pattern)\n",
    "\n",
    "        #: предобработка коллекции ключей, среди которых производится поиск\n",
    "        self._documents_tokenized = [\n",
    "            self._preprocess_one_sentence(doc, self._stopwords)\n",
    "            for doc in documents\n",
    "        ]\n",
    "\n",
    "        self._bm25_index = BM25Okapi(self._documents_tokenized)\n",
    "\n",
    "    def _preprocess_one_sentence(self, sentence: str, stopwords: set) -> list:\n",
    "        \"\"\"Предобрабатывает одно предложение.\n",
    "\n",
    "        Приводит к нижнему регистру, убирает мусорные слова,\n",
    "        оставляет слова, подходящиие под regex, лемматизирует.\n",
    "\n",
    "        Args:\n",
    "            sentence: предложение в формате строки (ключ для поиска).\n",
    "            stopwords: слова, убираемые из предложения.\n",
    "\n",
    "        Returns:\n",
    "            result: предобработанное предложение или пустой список.\n",
    "        \"\"\"\n",
    "        if isinstance(sentence, str):\n",
    "            regex_words = re.findall(self._TOKEN_PATTERN, sentence.lower())\n",
    "            clear_words = [\n",
    "                self._lemmatizer.parse(token.strip())[0].normal_form\n",
    "                for token in regex_words\n",
    "                if token not in stopwords\n",
    "            ]\n",
    "            return clear_words\n",
    "        return []\n",
    "\n",
    "    def make_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        top: int,\n",
    "        threshold: int | float | None = None\n",
    "    ) -> list[tuple[int, float]]:\n",
    "        \"\"\"Делает запрос к базе знаний.\n",
    "\n",
    "        Args:\n",
    "            query: текстовый запрос к базе знаний.\n",
    "            top: количество возвращаемых документов.\n",
    "            threshold: трешхолд.\n",
    "\n",
    "        Returns:\n",
    "            Список кортежей в формате: [(индекс документа, релевантность), ...].\n",
    "            релевантность - значение Okapi BM25.\n",
    "        \"\"\"\n",
    "\n",
    "        top = min(top, len(self._documents_tokenized))\n",
    "        query_tokenized = self._preprocess_one_sentence(query, self._stopwords)\n",
    "\n",
    "        scores = self._bm25_index.get_scores(query_tokenized)\n",
    "\n",
    "        top_k = np.argsort(scores)[::-1][:top]\n",
    "\n",
    "        result = [(i, scores[i]) for i in top_k]\n",
    "        if threshold is not None:\n",
    "            result = [elem for elem in result if elem[1] > threshold]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class EmbeddingRetriever(RetrieverInterface):\n",
    "    \"\"\"Семантический поиск по базе знаний.\n",
    "\n",
    "    Поиск с использованием эмбеддингов.\n",
    "    Модель для эмбеддингов должна быть совместима с `SentenceTransformers`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedder_path: str,\n",
    "                 documents: Iterable[str],\n",
    "                 device: torch.device | str) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedder_path: путь к модели для эмбеддингов.\n",
    "            documents: коллекция ключей, среди которых производится поиск.\n",
    "            device: девайс.\n",
    "        \"\"\"\n",
    "        self._embedder = SentenceTransformer(\n",
    "            model_name_or_path=embedder_path,\n",
    "            device=device,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        self._documents_embeddings = self._embedder.encode(\n",
    "            documents,\n",
    "            convert_to_tensor=True\n",
    "        )\n",
    "        self._documents_embeddings = self._documents_embeddings.to(device)\n",
    "        self._device = device\n",
    "\n",
    "    def make_query(\n",
    "        self,\n",
    "        query: str,\n",
    "        top: int,\n",
    "        threshold: int | None = None\n",
    "    ) -> list[tuple[int, float]]:\n",
    "        \"\"\"Делает запрос к базе знаний.\n",
    "\n",
    "        Args:\n",
    "            query: текстовый запрос к базе знаний.\n",
    "            top: количество возвращаемых документов.\n",
    "            threshold: трешхолд.\n",
    "\n",
    "        Returns:\n",
    "            Список кортежей в формате: [(индекс документа, релевантность), ...].\n",
    "            релевантность - косинусная близость.\n",
    "        \"\"\"\n",
    "        top = min(top, len(self._documents_embeddings))\n",
    "\n",
    "        query_emb = self._embedder.encode(query, convert_to_tensor=True)\n",
    "        query_emb = query_emb.to(self._device)\n",
    "\n",
    "        similarity_scores = util.semantic_search(\n",
    "            query_emb,\n",
    "            self._documents_embeddings,\n",
    "            top_k=top\n",
    "        )[0]\n",
    "\n",
    "        result = [(dct['corpus_id'], dct['score']) for dct in similarity_scores]\n",
    "\n",
    "        if threshold is not None:\n",
    "            result = [elem for elem in result if elem[1] > threshold]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class HybridRetriever(RetrieverInterface):\n",
    "    \"\"\"Гибридный/ансамблевый поиск по базе знаний.\n",
    "\n",
    "    Ранжирует и объединяет результаты произвольного\n",
    "    количества поисковиков, используя алгоритм RRF.\n",
    "\n",
    "    метод `make_query()` каждого из поисковиков должен\n",
    "    возвращать результаты в формате:\n",
    "        - [(doc_index, score), ...]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 retrievers: list,\n",
    "                 weights: list[float],\n",
    "                 documents: Iterable[str],\n",
    "                 thresholds: list | None) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            retrievers: список заранее созданных поисковиков.\n",
    "            weights: веса поисковиков.\n",
    "            documents: коллекция ключей, среди которых производится поиск.\n",
    "            thresholds: трешхолды поисковиков.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(retrievers) == len(weights), \"Количество поисковиков и весов должно совпадать\"\n",
    "        assert sum(weights) - 1 <= 0.0001, \"Сумма весов поисковиков должна быть равна 1\"\n",
    "        assert not sum(weights) - 1 > 0.0001, \"Сумма весов поисковиков не может быть больше 1\"\n",
    "        assert all(x >= 0 for x in weights), \"Значения весов не могут быть отрицательными\"\n",
    "\n",
    "        if thresholds is not None:\n",
    "            assert len(thresholds) == len(retrievers), \"Количество поисковиков и порогов должно совпадать\"\n",
    "            self._thresholds = thresholds\n",
    "        else:\n",
    "            self._thresholds = [None] * len(retrievers)\n",
    "\n",
    "        self._retrievers = retrievers\n",
    "        self._weights = weights\n",
    "\n",
    "    def _reciprocal_rank_fusion(\n",
    "        self,\n",
    "        scores: list[list[tuple[int, float]]],\n",
    "        weights: list[float]\n",
    "    ) -> list[tuple[int, float]]:\n",
    "        \"\"\"Объединяет результаты нескольких поисковиков.\n",
    "\n",
    "        Args:\n",
    "            scores: результаты индивидуальных поисковиков.\n",
    "            weights: веса поисковиков.\n",
    "        \"\"\"\n",
    "        assert len(scores) == len(weights), \"Количество поисковиков и весов должно совпадать\"\n",
    "\n",
    "        final_scores = defaultdict(float)\n",
    "\n",
    "        for idx, (score_list, weight) in enumerate(zip(scores, weights)):\n",
    "            for rank, (doc_index, score) in enumerate(score_list):\n",
    "                final_scores[doc_index] += weight / (rank + 1)\n",
    "\n",
    "        final_score_list = [\n",
    "            (doc_index, score)\n",
    "            for doc_index, score in final_scores.items()\n",
    "        ]\n",
    "        final_score_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        return final_score_list\n",
    "\n",
    "    def make_query(self,\n",
    "                   query: str,\n",
    "                   top: int) -> list[tuple[int, float]]:\n",
    "        \"\"\"Делает запрос к базе знаний.\n",
    "\n",
    "        Args:\n",
    "            query: текстовый запрос к базе знаний.\n",
    "            top: количество возвращаемых документов.\n",
    "\n",
    "        Returns:\n",
    "            Список кортежей в формате: [(индекс документа, релевантность), ...].\n",
    "            релевантность - reciprocal rank fusion score.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for retriever, threshold in zip(self._retrievers, self._thresholds):\n",
    "            scores.append(retriever.make_query(query, top, threshold))\n",
    "\n",
    "        fused_scores = self._reciprocal_rank_fusion(scores, self._weights)\n",
    "        return fused_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6420f-daa8-43c2-ac48-5e4ee9acc70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def squeeze_retrieved(\n",
    "    retriever_result: list[tuple[int, float]],\n",
    "    dataframe: pd.DataFrame,\n",
    "    df_col_index: int = 1\n",
    ") -> list[tuple[int, float]]:\n",
    "    \"\"\"Из нескольких услуг с одинаковым кодом\n",
    "    оставляет одну с наибольшим скором.\n",
    "\n",
    "    Args:\n",
    "        retriever_result: результаты поиска.\n",
    "        dataframe: датафрейм с услугами и кодами.\n",
    "        df_col_index: индекс колонки с кодом услуги в датафрейме.\n",
    "\n",
    "    Returns:\n",
    "        Список кортежей в формате:\n",
    "        [\n",
    "            (индекс документа, релевантность, КОД УСЛУГИ),\n",
    "            ...\n",
    "        ].\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for elem in retriever_result:\n",
    "        # получаем словарь вида {'КОД УСЛУГИ': (score, index), ...}\n",
    "        key = dataframe.iloc[elem[0], df_col_index]\n",
    "        if key not in result:\n",
    "            result[key] = elem\n",
    "        else:\n",
    "            _, old_value = result[key]\n",
    "            new_value = elem[1]\n",
    "            if new_value > old_value:\n",
    "                result[key] = elem\n",
    "\n",
    "    # распаковка\n",
    "    result = [(value[0], value[1], key) for key, value in result.items()]\n",
    "\n",
    "    # сортировка по итоговому значению скора\n",
    "    result = sorted(result, key=lambda item: -item[1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdae03-4cbc-4a9b-9a59-c2ebec776324",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61d4c9-321b-44e4-90ce-96f9433cb16c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Mistral, Vikhr, Mistral Q, Vikhr Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36864be6-5a66-4a0d-95ab-dc340609bef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "class AnswerGenerator:\n",
    "    \"\"\"Класс для использования LLM через vLLM.\n",
    "\n",
    "    Работает также с квантизованными моделями GGUF.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 gpu_memory_utilization,\n",
    "                 max_model_len=8192):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_path: путь к LLM в файловой системе.\n",
    "        \"\"\"\n",
    "        self.model = LLM(\n",
    "            model=model_path,\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "            max_model_len=max_model_len\n",
    "        )\n",
    "\n",
    "    def get_answer(\n",
    "        self,\n",
    "        params,\n",
    "        query: str,\n",
    "        contexts: list[str] | None = None,\n",
    "        system_prompt: str | None = None\n",
    "    ) -> tuple[str, list[dict[str, str]]]:\n",
    "        \"\"\"Генерирует ответ от LLM, переданной в __init__.\n",
    "\n",
    "        Args:\n",
    "            params: параметры, задаются через SamplingParams.\n",
    "            query: запрос.\n",
    "            contexts: контексты, которые необходимо учесть.\n",
    "            system_prompt: системный промпт.\n",
    "\n",
    "        Returns:\n",
    "            answer: ответ.\n",
    "            conversation: история диалога.\n",
    "        \"\"\"\n",
    "\n",
    "        if contexts is None:\n",
    "            contexts = ['']\n",
    "\n",
    "        combined_context = '\\n'.join(\n",
    "            [\n",
    "                f\"Контекст {i + 1}: {ctx}\"\n",
    "                for i, ctx in enumerate(contexts)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if system_prompt is None:\n",
    "            system_prompt = (\n",
    "                \"Ты - интеллектуальный ассистент, отвечающий вежливо на языке запроса. \" \n",
    "                \"Для ответа на вопросы ты всегда учитываешь и используешь переданный Контекст.\"\n",
    "            )\n",
    "\n",
    "        prompt = f\"Контекст: {combined_context}\\nВопрос: {query}:\"\n",
    "\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        outputs = self.model.chat(conversation, params)\n",
    "\n",
    "        answer = ''\n",
    "        for output in outputs:\n",
    "            answer += output.outputs[0].text\n",
    "            conversation.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": output.outputs[0].text\n",
    "            })\n",
    "\n",
    "        return answer, conversation\n",
    "\n",
    "    def chat_endlessly(\n",
    "        self,\n",
    "        params,\n",
    "        system_prompt: str | None = None\n",
    "    ) -> list[dict[str, str]]:\n",
    "        \"\"\"Бесконечный чат с LLM, переданной в __init__.\n",
    "\n",
    "        Args:\n",
    "            params: параметры, задаются через SamplingParams.\n",
    "            system_prompt: системный промпт.\n",
    "\n",
    "        Returns:\n",
    "            conversation: история диалога.\n",
    "        \"\"\"\n",
    "        if system_prompt is None:\n",
    "            system_prompt = \"Ты - интеллектуальный ассистент, отвечающий вежливо на языке запроса.\"\n",
    "\n",
    "        conversation = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "        while True:\n",
    "            query = input(\">>> \")\n",
    "            if not query:\n",
    "                break\n",
    "            if len(conversation) >= 13:\n",
    "                conversation = [conversation[0]] + conversation[-6:]\n",
    "\n",
    "            conversation.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "            outputs = self.model.chat(conversation, params)\n",
    "\n",
    "            for output in outputs:\n",
    "                answer = output.outputs[0].text\n",
    "\n",
    "                conversation.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": answer\n",
    "                })\n",
    "\n",
    "                print(answer)\n",
    "\n",
    "        return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf6c25-0c35-4781-b85a-375d26bb9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(\n",
    "    temperature=0.25,\n",
    "    max_tokens=1024,\n",
    "    logprobs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b719339-535a-4caa-b036-ac75a32c9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_answer_and_metrics_from_mistral(\n",
    "    model,\n",
    "    params,\n",
    "    query: str,\n",
    "    contexts: list[str] | None = None,\n",
    "    system_prompt: str | None = None\n",
    ") -> tuple:\n",
    "    \"\"\"docstring\n",
    "    \n",
    "    Доступно для мистраля и вихря\n",
    "    \"\"\"\n",
    "\n",
    "    # context processing\n",
    "    if contexts is None:\n",
    "        contexts = ['']\n",
    "\n",
    "    context = '\\n'.join(\n",
    "        [\n",
    "            f\"Контекст {i + 1}: {ctx}\"\n",
    "            for i, ctx in enumerate(contexts)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # system_prompt processing\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"Ты - интеллектуальный ассистент, отвечающий вежливо на языке запроса. \"\n",
    "            \"Для ответа на вопросы ты всегда учитываешь и используешь переданный [CONTEXT]. \"\n",
    "        )\n",
    "\n",
    "    prompt = f\"[CONTEXT]{context}\\n[QUESTION]{query}\"\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    # generating answer\n",
    "    start_time = time.time()\n",
    "    outputs = model.chat(conversation, params)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    answer = outputs[0].outputs[0].text\n",
    "    logprobs = outputs[0].outputs[0].logprobs\n",
    "    token_ids = outputs[0].outputs[0].token_ids\n",
    "\n",
    "    metrics = GeneratorMetrics(logprobs, token_ids)\n",
    "\n",
    "    perplexity = metrics.calculate_perplexity()\n",
    "    mean_token_probability = metrics.calculate_mean_token_probability()\n",
    "    mean_token_entropy = metrics.calculate_mean_token_entropy()\n",
    "\n",
    "    return (\n",
    "        query,\n",
    "        contexts,\n",
    "        answer,\n",
    "        perplexity,\n",
    "        mean_token_probability,\n",
    "        mean_token_entropy,\n",
    "        total_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb432b5e-2511-4659-a3e0-e78b70f4d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def rerank_get_answer_and_metrics_from_vikhr(\n",
    "    model,\n",
    "    params,\n",
    "    query: str,\n",
    "    contexts: list[str] | None = None,\n",
    "    system_prompt: str | None = None\n",
    ") -> tuple:\n",
    "    \"\"\"docstring\n",
    "\n",
    "    Доступно только для вихря\n",
    "    \"\"\"\n",
    "\n",
    "    # context processing\n",
    "    if contexts is None:\n",
    "        contexts = ['']\n",
    "\n",
    "    documents = [\n",
    "        {\"doc_id\": i, \"content\": v}\n",
    "        for i, v in enumerate(contexts)\n",
    "    ]\n",
    "\n",
    "    # system_prompt processing\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"Your task is to answer the user's questions \"\n",
    "            \"using only the information from the provided documents.\"\n",
    "            \"Give two answers to each question: \"\n",
    "            \"one with a list of relevant document identifiers \"\n",
    "            \"and the second with the answer to the question itself, \"\n",
    "            \"using documents with these identifiers.\"\n",
    "        )\n",
    "\n",
    "    conversation = [\n",
    "        {'role': 'system', 'content': system_prompt},\n",
    "        {'role': 'documents', 'content': json.dumps(documents, ensure_ascii=False)},\n",
    "        {'role': 'user', 'content': query}\n",
    "    ]\n",
    "\n",
    "    # reranking documents\n",
    "    relevant_indices = model.chat(conversation, params)[0].outputs[0].text\n",
    "\n",
    "    # generating answer\n",
    "    start_time = time.time()\n",
    "    outputs = model.chat(\n",
    "        conversation + [{'role': 'assistant', 'content': relevant_indices}],\n",
    "        params\n",
    "    )\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    answer = outputs[0].outputs[0].text\n",
    "    logprobs = outputs[0].outputs[0].logprobs\n",
    "    token_ids = outputs[0].outputs[0].token_ids\n",
    "\n",
    "    metrics = GeneratorMetrics(logprobs, token_ids)\n",
    "\n",
    "    perplexity = metrics.calculate_perplexity()\n",
    "    mean_token_probability = metrics.calculate_mean_token_probability()\n",
    "    mean_token_entropy = metrics.calculate_mean_token_entropy()\n",
    "\n",
    "    return (\n",
    "        query,\n",
    "        contexts,\n",
    "        answer,\n",
    "        perplexity,\n",
    "        mean_token_probability,\n",
    "        mean_token_entropy,\n",
    "        total_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a90431-8f43-4039-91de-82e6fcf4b007",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Gemma, Saiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935c51c-e7a8-4200-b6ae-8ce0480e26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "with open('uri_saiga_mistral.txt', 'r') as file:\n",
    "    MODEL_URI = file.read()\n",
    "\n",
    "\n",
    "# подгрузим из конфига\n",
    "with open('config_saiga_mistral.json', 'r') as openfile:\n",
    "    json_data = json.load(openfile)\n",
    "\n",
    "json_data['dataframe_records'][0]['kwargs']['temperature'] = 0.25\n",
    "json_data['dataframe_records'][0]['kwargs']['max_tokens'] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40de14a-e755-4183-b010-e3afaf4ddd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_saiga_mistral(data: dict) -> str:\n",
    "    \"\"\"Отправляет POST-запрос с данными на предсказание.\n",
    "\n",
    "    Args:\n",
    "        data: JSON-объект с массивом записей.\n",
    "\n",
    "    Returns:\n",
    "        Ответ от LLM Saiga-Mistral-7B-Instruct-v0.2.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        MODEL_URI,\n",
    "        json=data,\n",
    "    )\n",
    "\n",
    "    answer = ''\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        if 'predictions' in response_data:\n",
    "            check_text = response_data['predictions'][0]\n",
    "            answer = check_text['answer']\n",
    "            return answer\n",
    "        else:\n",
    "            print(f'Ошибка в ответе: {response_data}')\n",
    "            return answer\n",
    "    else:\n",
    "        print(f'Ошибка: {response.status_code} - {response.json()}')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5177c7-9c2e-4f70-84c3-69db724cf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_answer_from_saiga(\n",
    "    dct: dict,\n",
    "    query: str,\n",
    "    contexts: list[str] | None = None,\n",
    "    system_prompt: str | None = None\n",
    ") -> tuple:\n",
    "    \"\"\"docstring\n",
    "    \"\"\"\n",
    "\n",
    "    # context processing\n",
    "    if contexts is None:\n",
    "        contexts = ['']\n",
    "\n",
    "    context = '\\n'.join(\n",
    "        [\n",
    "            f\"Контекст {i + 1}: {ctx}\"\n",
    "            for i, ctx in enumerate(contexts)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # system_prompt processing\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"Ты - интеллектуальный ассистент, отвечающий вежливо на языке запроса. \"\n",
    "            \"Для ответа на вопросы ты всегда учитываешь и используешь переданный контекст. \"\n",
    "        )\n",
    "\n",
    "    dct['dataframe_records'][0]['system_prompt'] = system_prompt\n",
    "    dct['dataframe_records'][0]['query'] = query\n",
    "    dct['dataframe_records'][0]['context'] = context\n",
    "\n",
    "    # generating answer\n",
    "    start_time = time.time()\n",
    "    answer = post_to_saiga_mistral(dct)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return answer, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbea0b-d7b7-4e69-83c2-d7896e9783b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "with open('uri_gemma.txt', 'r') as file:\n",
    "    MODEL_URI = file.read()\n",
    "\n",
    "# подгрузим из конфига\n",
    "with open('config_gemma.json', 'r') as openfile:\n",
    "    data_json = json.load(openfile)\n",
    "\n",
    "data_json['dataframe_records'][0]['config']['kwargs']['temperature'] = 0.25\n",
    "data_json['dataframe_records'][0]['config']['kwargs']['max_tokens'] = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad8d4f-1d80-4141-92d5-d1418005468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_gemma(data: dict) -> str:\n",
    "    \"\"\"Отправляет POST-запрос с данными на предсказание.\n",
    "\n",
    "    Args:\n",
    "        data: JSON-объект с массивом записей.\n",
    "\n",
    "    Returns:\n",
    "        Ответ от LLM Gemma-2-9b-it.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.post(\n",
    "        MODEL_URI,\n",
    "        json=data,\n",
    "    )\n",
    "\n",
    "    answer = ''\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        if 'predictions' in response_data:\n",
    "            check_text = response_data['predictions'][0]\n",
    "            answer = check_text['response'][0]\n",
    "            return answer\n",
    "        else:\n",
    "            print(f'Ошибка в ответе: {response_data}')\n",
    "            return answer\n",
    "    else:\n",
    "        print(f'Ошибка: {response.status_code} - {response.json()}')\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff2449-b7f5-4efc-ad32-e946aa2c3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def get_answer_from_gemma(\n",
    "    dct: dict,\n",
    "    query: str,\n",
    "    contexts: list[str] | None = None,\n",
    "    system_prompt: str | None = None\n",
    ") -> tuple:\n",
    "    \"\"\"docstring\n",
    "    \"\"\"\n",
    "\n",
    "    # context processing\n",
    "    if contexts is None:\n",
    "        contexts = ['']\n",
    "\n",
    "    context = '\\n'.join(\n",
    "        [\n",
    "            f\"Контекст {i + 1}: {ctx}\"\n",
    "            for i, ctx in enumerate(contexts)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # system_prompt processing\n",
    "    if system_prompt is None:\n",
    "        system_prompt = (\n",
    "            \"Ты - интеллектуальный ассистент, отвечающий вежливо на языке запроса. \"\n",
    "            \"Для ответа на вопросы ты всегда учитываешь и используешь переданное поле 'Контекст'. \"\n",
    "        )\n",
    "\n",
    "    template = (\n",
    "        f'Системный промпт: {system_prompt}\\n'\n",
    "        f'Контекст: {context}\\n'\n",
    "        f'Вопрос пользователя: {query}'\n",
    "    )\n",
    "\n",
    "    dct['dataframe_records'][0]['query'] = template\n",
    "\n",
    "    # generating answer\n",
    "    start_time = time.time()\n",
    "    answer = post_to_gemma(dct)\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return answer, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62ac38-1ba2-4eb9-bd63-4e9f87381331",
   "metadata": {
    "tags": []
   },
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120aef6-805f-4ea2-b782-940019722f8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### retriever metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03dd6d9-6d93-4125-8bf1-9d1208e7f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class RetrieverMetrics:\n",
    "    def __init__(self,\n",
    "                 y_true: Iterable,\n",
    "                 y_pred: Iterable,\n",
    "                 top: int):\n",
    "\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.top = top\n",
    "\n",
    "    def p_at_k(self, y_true: Iterable, y_pred: Iterable) -> float:\n",
    "        \"\"\"Computes precision at k.\n",
    "\n",
    "        Args:\n",
    "            y_true: A collection of all relevant labels.\n",
    "            y_pred: A collection of predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            score: precision at k score.\n",
    "        \"\"\"\n",
    "        if not y_pred:\n",
    "            return 0.0\n",
    "\n",
    "        count = 0\n",
    "        actual = set(y_true)\n",
    "        for elem in y_pred:\n",
    "            if elem in actual:\n",
    "                count += 1\n",
    "\n",
    "        score = count / len(y_pred)\n",
    "        return score\n",
    "\n",
    "    def compute_theo_max(self, top: int) -> float:\n",
    "        \"\"\"Computes theoretical maximum for AP@K.\n",
    "\n",
    "        Args:\n",
    "            top: The maximum number of predicted elements.\n",
    "\n",
    "        Returns:\n",
    "            score: theoretical_maximum for AP@K.\n",
    "        \"\"\"\n",
    "        theo_array = [0 for i in range(top)]\n",
    "        theo_array[0] = 1\n",
    "        theo_true = [1]\n",
    "        score = 0.0\n",
    "        for i in range(1, top + 1):\n",
    "            score += self.p_at_k(theo_true, theo_array[:i])\n",
    "\n",
    "        score /= top\n",
    "        return score\n",
    "\n",
    "    def ap_at_k(self,\n",
    "                y_true: Iterable,\n",
    "                y_pred: Iterable,\n",
    "                top: int) -> tuple[float]:\n",
    "        \"\"\"Computes average precision at k.\n",
    "\n",
    "        Args:\n",
    "            y_true: A collection of all relevant labels.\n",
    "            y_pred: Iterable: A collection of predicted labels.\n",
    "            top: The maximum number of predicted elements.\n",
    "\n",
    "        Returns:\n",
    "            score: AP@K score.\n",
    "            norm_score: normalized score.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        actual_length = min(top, len(y_pred))\n",
    "\n",
    "        for i in range(1, actual_length + 1):\n",
    "            score += self.p_at_k(y_true, y_pred[:i])\n",
    "\n",
    "        theo_max = self.compute_theo_max(actual_length)\n",
    "        norm_score = score / theo_max\n",
    "\n",
    "        norm_score /= actual_length\n",
    "        score /= actual_length\n",
    "        return score, norm_score\n",
    "\n",
    "    def map_at_k(self,\n",
    "                 y_true: Iterable,\n",
    "                 y_pred: Iterable,\n",
    "                 top) -> tuple[float]:\n",
    "        \"\"\"Computes mean average precision at k.\n",
    "\n",
    "        Args:\n",
    "            y_true: A collection of collections of all relevant labels.\n",
    "            y_pred: A collection of collections of predicted labels.\n",
    "            top: The maximum number of predicted elements.\n",
    "\n",
    "        Returns:\n",
    "            score: MAP@K score.\n",
    "            norm_score: normalized score.\n",
    "        \"\"\"\n",
    "\n",
    "        aps_at_k = [\n",
    "            self.ap_at_k(actual, pred, top)\n",
    "            for actual, pred in zip(y_true, y_pred)\n",
    "        ]\n",
    "\n",
    "        score = np.mean([elem[0] for elem in aps_at_k])\n",
    "        norm_score = np.mean([elem[1] for elem in aps_at_k])\n",
    "\n",
    "        return score, norm_score\n",
    "\n",
    "    def rr_at_k(self, y_true: Iterable, y_pred: Iterable) -> float:\n",
    "        \"\"\"Computes reciprocal rank at k.\n",
    "\n",
    "        Args:\n",
    "            y_true: A collection of all relevant labels.\n",
    "            y_pred: A collection of predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            score: RR@K score.\n",
    "        \"\"\"\n",
    "        if not y_pred:\n",
    "            return 0.0\n",
    "\n",
    "        actual = set(y_true)\n",
    "        score = 0.0\n",
    "        for idx, elem in enumerate(y_pred):\n",
    "            if elem in actual:\n",
    "                score = 1 / (idx + 1)\n",
    "                return score\n",
    "        return score\n",
    "\n",
    "    def mrr_at_k(self, y_true: Iterable, y_pred: Iterable) -> float:\n",
    "        \"\"\"Computes reciprocal rank at k.\n",
    "\n",
    "        Args:\n",
    "            y_true: A collection of collections of all relevant labels.\n",
    "            y_pred: A collection of collections of predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            score: MRR@K score.\n",
    "        \"\"\"\n",
    "        score = np.mean([\n",
    "            self.rr_at_k(actual, pred)\n",
    "            for actual, pred in zip(self.y_true, self.y_pred)\n",
    "            ]\n",
    "        )\n",
    "        return score\n",
    "\n",
    "    def report(self) -> tuple[float]:\n",
    "        map_k, n_map_k = self.map_at_k(self.y_true, self.y_pred, self.top)\n",
    "        mrr_k = self.mrr_at_k(self.y_true, self.y_pred)\n",
    "        return map_k, n_map_k, mrr_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40248a12-01c4-4693-90d5-5e088e5ade04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LLM metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe823a-4cd6-42e9-bd86-45544ac1dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GeneratorMetrics:\n",
    "    \"\"\"Считает метрики ответа LLM.\n",
    "\n",
    "    Точно работает с фреймворком vLLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logprobs: list[dict], token_ids: tuple[int]) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logprobs: лог-вероятности токенов ответа.\n",
    "            token_ids: индексы токенов ответа.\n",
    "        \"\"\"\n",
    "        self.logprobs = logprobs\n",
    "        self.token_ids = token_ids\n",
    "        self.decoded_logprobs = [\n",
    "            dct[key].logprob for dct, key in zip(logprobs, token_ids)\n",
    "        ]\n",
    "        self.probs = np.exp(self.decoded_logprobs)\n",
    "\n",
    "    def calculate_perplexity(self) -> float:\n",
    "        \"\"\"Считает перплексию ответа LLM.\n",
    "\n",
    "        Returns:\n",
    "            perplexity: перплексия.\n",
    "        \"\"\"\n",
    "        dtype = np.float64\n",
    "        perplexity = np.exp(\n",
    "            -np.mean(self.decoded_logprobs, dtype=dtype),\n",
    "            dtype=dtype\n",
    "        )\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_mean_token_probability(self) -> float:\n",
    "        \"\"\"Считает среднюю вероятность токенов ответа LLM.\n",
    "\n",
    "        Returns:\n",
    "            mean_token_probability: средняя вероятность токенов ответа.\n",
    "        \"\"\"\n",
    "        dtype = np.float64\n",
    "        mean_token_probability = np.mean(self.probs, dtype=dtype)\n",
    "        return mean_token_probability\n",
    "\n",
    "    def calculate_sequence_probability(self) -> float:\n",
    "        \"\"\"Считает вероятность последовательности ответа LLM.\n",
    "\n",
    "        Returns:\n",
    "            sequence_probability: вероятность последовательности ответа.\n",
    "        \"\"\"\n",
    "        dtype = np.float64\n",
    "        sequence_probability = np.prod(self.probs, dtype=dtype)\n",
    "        return sequence_probability\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_logprob_dict(dct: dict) -> Iterable[float]:\n",
    "        \"\"\"Декодирует словарь лог-вероятностей.\n",
    "\n",
    "        Args:\n",
    "            dct: написать потом.\n",
    "\n",
    "        Returns:\n",
    "            result: написать потом.\n",
    "        \"\"\"\n",
    "        dtype = np.float64\n",
    "        result = np.array([v.logprob for _, v in dct.items()], dtype=dtype)\n",
    "        return result\n",
    "\n",
    "    def calculate_mean_token_entropy(self) -> float:\n",
    "        \"\"\"Считает среднюю энтропию токенов ответа LLM.\n",
    "\n",
    "        Returns:\n",
    "            mean_token_entropy: средняя энтропия ответа.\n",
    "        \"\"\"\n",
    "        dtype = np.float64\n",
    "        sum_entropy = 0.0\n",
    "\n",
    "        for elem in self.logprobs:\n",
    "            log_p = GeneratorMetrics.decode_logprob_dict(elem)\n",
    "            sum_entropy -= np.sum(log_p * np.exp(log_p), dtype=dtype)\n",
    "\n",
    "        mean_token_entropy = np.mean(sum_entropy, dtype=dtype)\n",
    "        return mean_token_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cd773-07ef-4e17-890f-e36395d1f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = # model_path\n",
    "\n",
    "scorer = BERTScorer(model_type=model_path, num_layers=24, device=device, lang=\"ru\")\n",
    "\n",
    "ref_answers = question_answer['Ответ'].tolist()\n",
    "\n",
    "\n",
    "def calculate_bertscore(llm_answers) -> dict[str, list[float]]:\n",
    "    llm_answers = llm_answers['answer'].tolist()\n",
    "    P, R, F1 = scorer.score(cands=llm_answers, refs=ref_answers)\n",
    "\n",
    "    result = {\n",
    "        'precision': P.tolist(),\n",
    "        'recall': R.tolist(),\n",
    "        'f1_score': F1.tolist(),\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3f827-0362-4893-9d55-407e73977fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a9eb8-2fb6-44ac-a9e8-d6cc7b930eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import json5\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# грузим конфиг\n",
    "with open('config_ruadapt_qwen.json', 'r') as file:\n",
    "    json_file = json.load(file)\n",
    "\n",
    "\n",
    "# конфиг в формате: {'BASE_URL': value, 'API_KEY': value, 'MODEL': value}\n",
    "client = OpenAI(\n",
    "    base_url=json_file['BASE_URL'],\n",
    "    api_key=json_file['API_KEY'],\n",
    ")\n",
    "\n",
    "MODEL = json_file['MODEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61f60d-f3a4-4387-8efa-6b1f75700ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON schema for Structured Output\n",
    "output_format = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Правильность\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 2\n",
    "        },\n",
    "        \"Точность\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5},\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 2\n",
    "        },\n",
    "        \"Полнота\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5},\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 2\n",
    "        },\n",
    "        \"Релевантность вопросу\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5},\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 2\n",
    "        },\n",
    "        \"Соответствие контекстам\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 1},\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 2\n",
    "        },\n",
    "        \"Соответствие ответу эксперта\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 1},\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 2\n",
    "        },\n",
    "        \"Лучший ответ\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 1}\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"Правильность\",\n",
    "        \"Точность\",\n",
    "        \"Полнота\",\n",
    "        \"Релевантность вопросу\",\n",
    "        \"Соответствие контекстам\",\n",
    "        \"Соответствие ответу эксперта\",\n",
    "        \"Лучший ответ\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a2e59-df04-42a9-a8d7-d24a35241ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# критерии оценки\n",
    "criteria = \"\"\"Критерии оценки:\n",
    "1. Правильность ('верно' - верный ответ, 'неверно' - верный ответ, 'отказ' - отказ от ответа).\n",
    "2. Точность (0-5 баллов).\n",
    "3. Полнота (0-5 баллов).\n",
    "4. Релевантность вопросу (0-5 баллов).\n",
    "5. Соответствие контекстам (0 - не соответствует, 1 - соответствует).\n",
    "6. Соответствие ответу эксперта (0 - не соответствует, 1 - соответствует).\n",
    "7. Лучший ответ (0 или 1).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d680e0a-a575-4b3f-af0b-1834c6309fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for Structured Output\n",
    "def evaluate_one_pair_of_answers(\n",
    "    question: str,\n",
    "    reference: str,\n",
    "    contexts: str,\n",
    "    answers: list[str]\n",
    ") -> str:\n",
    "    \"\"\"Оценивает ответы с помощью RuadaptQwen2.5-32B-Pro-Beta.\n",
    "\n",
    "    Args:\n",
    "        question: вопрос.\n",
    "        reference: ответ эксперта.\n",
    "        contexts: контексты для ответа.\n",
    "        answers: возможные ответы.\n",
    "\n",
    "    Returns:\n",
    "        judgement - ответ от LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = f\"\"\"Ты - эксперт по оценке качества ответов.\n",
    "    Тебе будет предоставлен: вопрос, контексты к вопросу, ответ эксперта и два варианта возможных ответов на вопрос.\n",
    "    Оцени каждый из ответов по следующим критериям.\n",
    "    {criteria}\n",
    "    Обязательно, верни оценки в формате JSON, не добавляй символы Markdown:\n",
    "    {json.dumps(output_format, ensure_ascii=False, indent=2)}\n",
    "\n",
    "    Не добавляй никаких объяснений. Верни только чистый JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT = f\"ВОПРОС: {question}\\nКОНТЕКСТЫ: {contexts}\\nОТВЕТ ЭКСПЕРТА: {reference}\"\n",
    "\n",
    "    MESSAGES = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": PROMPT},\n",
    "    ]\n",
    "\n",
    "    for i, answer in enumerate(answers, start=1):\n",
    "        MESSAGES.append({\"role\": \"user\", \"content\": f\"{i}. {answer}\"})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=MESSAGES,\n",
    "        max_tokens=256,\n",
    "        temperature=0.01,\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"evaluate_responses\",\n",
    "                \"description\": \"Оценка ответов по заданным критериям.\",\n",
    "                \"parameters\": output_format\n",
    "            }\n",
    "        ],\n",
    "        function_call={\"name\": \"evaluate_responses\"}\n",
    "    )\n",
    "\n",
    "    judgement = response.choices[0].message.content\n",
    "    return judgement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52760a1c-87ad-4945-a979-391963354532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_two_llms(llm_one, llm_two):\n",
    "    assert len(llm_one) == len(llm_two)\n",
    "\n",
    "    scores_llm_one = {\n",
    "        'Правильность': [],\n",
    "        'Точность': [],\n",
    "        'Полнота': [],\n",
    "        'Релевантность вопросу': [],\n",
    "        'Соответствие контекстам': [],\n",
    "        'Соответствие ответу эксперта': [],\n",
    "        'Лучший ответ': []\n",
    "    }\n",
    "\n",
    "    scores_llm_two = {\n",
    "        'Правильность': [],\n",
    "        'Точность': [],\n",
    "        'Полнота': [],\n",
    "        'Релевантность вопросу': [],\n",
    "        'Соответствие контекстам': [],\n",
    "        'Соответствие ответу эксперта': [],\n",
    "        'Лучший ответ': []\n",
    "    }\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for i in range(len(llm_one)):\n",
    "        print(f\"{i + 1}/{len(llm_one)}\")\n",
    "\n",
    "        question = question_answer['Вопрос'][i]\n",
    "        reference = question_answer['Ответ'][i]\n",
    "\n",
    "        answers = [llm_one['answer'][i], llm_two['answer'][i]]\n",
    "\n",
    "        result = evaluate_one_pair_of_answers(\n",
    "            question,\n",
    "            reference,\n",
    "            contexts[i],\n",
    "            answers\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            json_data = json5.loads(result.strip(\"`\").strip(\"json\"))\n",
    "            for k, v in json_data.items():\n",
    "                if k == 'Лучший ответ':\n",
    "                    scores_llm_one[k].append(v)\n",
    "                    scores_llm_two[k].append(v)\n",
    "                else:\n",
    "                    scores_llm_one[k].append(v[0])\n",
    "                    scores_llm_two[k].append(v[1])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"{err}\")\n",
    "            index_error = (i, json_data)\n",
    "            errors.append(index_error)\n",
    "\n",
    "    return scores_llm_one, scores_llm_two, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d26830-3b6c-42e5-a0a6-8d14e6c14821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
